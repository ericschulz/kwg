for (k in 1:myjson$count){
#initialize data frame
d<-data.frame(x1=numeric(), x2=numeric(), y=numeric())
for (i in 2:9){
#enrionment
env<-myjson$records$data$envOrder[[k]]
#condition
condition<-myjson$records$data$condition[k]
#smooth or rough
if(condition==0){ee<-data.frame(smoothEnvironments[[env[i]+1]])}
if(condition==1){ee<-data.frame(roughEnvironments[[env[i]+1]])}
#scale
ee$y<-ee$y*myjson$records$data$scale[[k]][i]+5
#bind
d<-rbind(d, data.frame(x1=ee$x1, x2=ee$x2, y=ee$y))
}
#add round
d$round<-rep(1:8, each=64)
#add it
d$id<-k
#age
d$age<-myjson$records$data$age[k]
#condition
d$condition<-condition
#concatenate or create
if (k==1){dat<-d}
if (k>1){dat<-rbind(dat, d)}
}
#first 10 was us!
dat<-subset(dat, id>10)
#at least 7
dat<-subset(dat, age>=7)
#new ids
dat$id<-rep(1:160, each=64*8)
#get the RBF results
dbmt<-read.csv("bmt.csv")
dbmt$error<-ave(dbmt$par1, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$beta<-ave(dbmt$par2, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$tau<-ave(dbmt$par3, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$error<-ifelse(is.na(dbmt$error), mean(dbmt$error, na.rm=TRUE), dbmt$error)
dbmt$beta<-ifelse(is.na(dbmt$beta), mean(dbmt$beta, na.rm=TRUE), dbmt$beta)
dbmt$tau<-ifelse(is.na(dbmt$tau), mean(dbmt$tau, na.rm=TRUE), dbmt$tau)
dd<-ddply(dbmt, ~id,summarize, tau=mean(exp(tau)),
beta=mean(exp(beta)), error=mean(exp(error)))
write.csv(dd, "bmtucbparams.csv")
#lambda
dat$error<-rep(dbmt$error, each=64)
#beta
dat$beta<-rep(dbmt$beta, each=64)
#tau
dat$tau<-rep(dbmt$tau, each=64)
#frame to collect observations
dcollect<-data.frame(id=numeric(), age=numeric(), condition=numeric(), round=numeric(), trial=numeric(), x=numeric(), y=numeric(), z=numeric())
#loop through participants
for (nid in 1:160){
#loop through rounds
for (nround in 1:8){
#get parameters for participant on that round
dp<-subset(dat, round==nround & id==nid)
#random initialization as observation t=0
ind<-sample(1:64,1)
#X matrix
X<-as.matrix(dp[ind,1:2])
#y matrix
y<-as.matrix(dp[ind,3])
#X-start, i.e. all possible observations
Xstar<-as.matrix(dp[,1:2])
#get lambda
error<-exp(dp$error[1])
#get beta
beta<-exp(dp$beta[1])
#get tau
tau<-exp(dp$tau[1])
#create a parameter vector
parVec <- c(error)
#kernel is RBF
prevPost <- NULL
#loop through trials
for (trial in 1:25){
#output by GP with particular parameter settings
#don't forget mean centering and standardization
out<-bayesianMeanTracker(x=t(X[trial,]), y=(y[trial]-25)/50, theta=parVec, prevPost = prevPost)
prevPost<-out
#utility vector by UCB
utilityVec<-ucb(out, beta)
#avoid overflow
utilities <- utilityVec - max(utilityVec)
#softmaximization
p <- exp(utilities/tau)
#probabilities
p <- p/colSums(p)
#numerical overflow
p <- (pmax(p, 0.00001))
p <- (pmin(p, 0.99999))
#index is sampled proprotionally to softmaxed utitily vector
ind<-sample(1:64, 1, prob=p)
#bind X-observations
X<-rbind(X, as.matrix(dp[ind,1:2]))
#bind y-observations
y<-rbind(y, as.matrix(dp[ind,3]))
}
#dummy data frame
dummy<-data.frame(id=rep(nid, 26), age=rep(dp$age[1], 26), condition=rep(dp$condition[1], 26),
round=rep(nround, 26), trial=0:25, x=as.numeric(X[,1]), y=as.numeric(X[,2]),
z=as.numeric(y))
#bind them
dcollect<-rbind(dcollect, dummy)
}
}
#save csv of learning curves
write.csv(dcollect, "bmtrecoverydata.csv")
dat<-dcollect
#age groups are 7-8, 9-11, and adults
dat$agegroup<-ifelse(dat$age<9, "7-8", dat$age)
dat$agegroup<-ifelse(dat$age>=9 & dat$age <12, "9-11", dat$agegroup)
dat$agegroup<-ifelse(dat$age>18, ">18", dat$agegroup)
#conditions are smooth and rough
dat$Condition<-ifelse(dat$cond==1, "Rough", "Smooth")
se<-function(x){sd(x)/sqrt(length(x))}
#summarize by agegroup, trial, and condition
dd<-ddply(dat, ~agegroup+trial+Condition, summarize, mu=mean(z), se=se(z))
#rough and smooth
dd$Condition<-factor(dd$Condition, levels = c("Smooth", "Rough"))
#call it Age again
dd$Age<-dd$agegroup
#factors...
dd$Age<-factor(dd$agegroup, levels=c("7-8", "9-11", ">18"))
pd <- position_dodge(.1)
#palette
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
fontsize<-12
##Learning over trials by age and condition
p3<-ggplot(dd, aes(x=trial, y=mu, group=interaction(Condition, Age), col=Age, linetype=Condition)) +
#error bars
geom_errorbar(aes(ymin=mu-se, ymax=mu+se), width=.8, size = .5, position=pd, linetype=1) +
#line
geom_line(size=0.8, position=pd)+
#labs
ylab("Reward")+xlab("Trial")+
#scales
#scale_y_continuous(breaks=c(25,30,35,40,45))+
scale_x_continuous(breaks=seq(0,26,5))+
#theme
theme_minimal()+
#color scheme
scale_color_manual(values = cbPalette[c(7,6,1)])+
#theme
theme(text = element_text(size=fontsize,  family="sans")) +
#titlte
ggtitle("c) Learning curves")+
#theme
theme(legend.position="none", strip.background=element_blank(), legend.key=element_rect(color=NA))
p3
dopt<-expand.grid(x=0:7, y=0:7)
dat$chosen<-0
for (i in 1:nrow(dat)){
dat$chosen[i]<-which(dopt$x==dat$x[i] & dopt$y == dat$y[i])
}
head(dat, 200)
dfinal<-data.frame(id=dat$id, round=dat$round, trial=dat$trial,
x=dat$x, y=dat$y, z=dat$z, chosen=dat$chosen)
write.csv(dfinal, "recoverbmt.csv")
#Simulation of BMT data
#Eric Schulz, March 2018
#housekeeping
rm(list=ls())
#source of modeling code
source("models.R")
#packages
packages <- c('plyr', 'ggplot2', 'jsonlite', 'lsr', 'BayesFactor', 'matrixcalc')
#load them
lapply(packages, require, character.only = TRUE)
#two environments
environments1 <- fromJSON("kernelRough.json")
environments2 <- fromJSON("kernelSmooth.json")
#participants' data
myjson<-fromJSON("kwg.json")
#exctract environments
roughEnvironments <- lapply(environments1, FUN=function(x) matrix(as.numeric(unlist(x)), ncol=3, byrow=TRUE, dimnames=list(seq(1,64), c('x2', 'y', 'x1'))))
smoothEnvironments <- lapply(environments2, FUN=function(x) matrix(as.numeric(unlist(x)), ncol=3, byrow=TRUE, dimnames=list(seq(1,64), c('x2', 'y', 'x1' ))))
#loop through the counts
for (k in 1:myjson$count){
#initialize data frame
d<-data.frame(x1=numeric(), x2=numeric(), y=numeric())
for (i in 2:9){
#enrionment
env<-myjson$records$data$envOrder[[k]]
#condition
condition<-myjson$records$data$condition[k]
#smooth or rough
if(condition==0){ee<-data.frame(smoothEnvironments[[env[i]+1]])}
if(condition==1){ee<-data.frame(roughEnvironments[[env[i]+1]])}
#scale
ee$y<-ee$y*myjson$records$data$scale[[k]][i]+5
#bind
d<-rbind(d, data.frame(x1=ee$x1, x2=ee$x2, y=ee$y))
}
#add round
d$round<-rep(1:8, each=64)
#add it
d$id<-k
#age
d$age<-myjson$records$data$age[k]
#condition
d$condition<-condition
#concatenate or create
if (k==1){dat<-d}
if (k>1){dat<-rbind(dat, d)}
}
#first 10 was us!
dat<-subset(dat, id>10)
#at least 7
dat<-subset(dat, age>=7)
#new ids
dat$id<-rep(1:160, each=64*8)
#get the RBF results
dbmt<-read.csv("bmt.csv")
dbmt$error<-ave(dbmt$par1, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$beta<-ave(dbmt$par2, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$tau<-ave(dbmt$par3, dbmt$id, FUN=function(x){mean(x[exp(x)<5])})
dbmt$error<-ifelse(is.na(dbmt$error), mean(dbmt$error, na.rm=TRUE), dbmt$error)
dbmt$beta<-ifelse(is.na(dbmt$beta), mean(dbmt$beta, na.rm=TRUE), dbmt$beta)
dbmt$tau<-ifelse(is.na(dbmt$tau), mean(dbmt$tau, na.rm=TRUE), dbmt$tau)
dd<-ddply(dbmt, ~id,summarize, tau=mean(exp(tau)),
beta=mean(exp(beta)), error=mean(exp(error)))
write.csv(dd, "bmtucbparams.csv")
#lambda
dat$error<-rep(dbmt$error, each=64)
#beta
dat$beta<-rep(dbmt$beta, each=64)
#tau
dat$tau<-rep(dbmt$tau, each=64)
#frame to collect observations
dcollect<-data.frame(id=numeric(), age=numeric(), condition=numeric(), round=numeric(), trial=numeric(), x=numeric(), y=numeric(), z=numeric())
#loop through participants
for (nid in 1:160){
#loop through rounds
for (nround in 1:8){
#get parameters for participant on that round
dp<-subset(dat, round==nround & id==nid)
#random initialization as observation t=0
ind<-sample(1:64,1)
#X matrix
X<-as.matrix(dp[ind,1:2])
#y matrix
y<-as.matrix(dp[ind,3])
#X-start, i.e. all possible observations
Xstar<-as.matrix(dp[,1:2])
#get lambda
error<-exp(dp$error[1])
#get beta
beta<-exp(dp$beta[1])
#get tau
tau<-exp(dp$tau[1])
#create a parameter vector
parVec <- c(error)
#kernel is RBF
prevPost <- NULL
#loop through trials
for (trial in 1:25){
#output by GP with particular parameter settings
#don't forget mean centering and standardization
out<-bayesianMeanTracker(x=X[trial,], y=(y[trial]-25)/50, theta=parVec, prevPost = prevPost)
prevPost<-out
#utility vector by UCB
utilityVec<-ucb(out, beta)
#avoid overflow
utilities <- utilityVec - max(utilityVec)
#softmaximization
p <- exp(utilities/tau)
#probabilities
p <- p/colSums(p)
#numerical overflow
p <- (pmax(p, 0.00001))
p <- (pmin(p, 0.99999))
#index is sampled proprotionally to softmaxed utitily vector
ind<-sample(1:64, 1, prob=p)
#bind X-observations
X<-rbind(X, as.matrix(dp[ind,1:2]))
#bind y-observations
y<-rbind(y, as.matrix(dp[ind,3]))
}
#dummy data frame
dummy<-data.frame(id=rep(nid, 26), age=rep(dp$age[1], 26), condition=rep(dp$condition[1], 26),
round=rep(nround, 26), trial=0:25, x=as.numeric(X[,1]), y=as.numeric(X[,2]),
z=as.numeric(y))
#bind them
dcollect<-rbind(dcollect, dummy)
}
}
#save csv of learning curves
write.csv(dcollect, "bmtrecoverydata.csv")
dat<-dcollect
#age groups are 7-8, 9-11, and adults
dat$agegroup<-ifelse(dat$age<9, "7-8", dat$age)
dat$agegroup<-ifelse(dat$age>=9 & dat$age <12, "9-11", dat$agegroup)
dat$agegroup<-ifelse(dat$age>18, ">18", dat$agegroup)
#conditions are smooth and rough
dat$Condition<-ifelse(dat$cond==1, "Rough", "Smooth")
se<-function(x){sd(x)/sqrt(length(x))}
#summarize by agegroup, trial, and condition
dd<-ddply(dat, ~agegroup+trial+Condition, summarize, mu=mean(z), se=se(z))
#rough and smooth
dd$Condition<-factor(dd$Condition, levels = c("Smooth", "Rough"))
#call it Age again
dd$Age<-dd$agegroup
#factors...
dd$Age<-factor(dd$agegroup, levels=c("7-8", "9-11", ">18"))
pd <- position_dodge(.1)
#palette
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
fontsize<-12
##Learning over trials by age and condition
p3<-ggplot(dd, aes(x=trial, y=mu, group=interaction(Condition, Age), col=Age, linetype=Condition)) +
#error bars
geom_errorbar(aes(ymin=mu-se, ymax=mu+se), width=.8, size = .5, position=pd, linetype=1) +
#line
geom_line(size=0.8, position=pd)+
#labs
ylab("Reward")+xlab("Trial")+
#scales
#scale_y_continuous(breaks=c(25,30,35,40,45))+
scale_x_continuous(breaks=seq(0,26,5))+
#theme
theme_minimal()+
#color scheme
scale_color_manual(values = cbPalette[c(7,6,1)])+
#theme
theme(text = element_text(size=fontsize,  family="sans")) +
#titlte
ggtitle("c) Learning curves")+
#theme
theme(legend.position="none", strip.background=element_blank(), legend.key=element_rect(color=NA))
p3
dopt<-expand.grid(x=0:7, y=0:7)
dat$chosen<-0
for (i in 1:nrow(dat)){
dat$chosen[i]<-which(dopt$x==dat$x[i] & dopt$y == dat$y[i])
}
head(dat, 200)
dfinal<-data.frame(id=dat$id, round=dat$round, trial=dat$trial,
x=dat$x, y=dat$y, z=dat$z, chosen=dat$chosen)
write.csv(dfinal, "recoverbmt.csv")
seq(-127,127, length.out = 8)
rm(list=ls()) #house keeping
#load packages
packages <- c('plyr', 'jsonlite', 'DEoptim', "matrixcalc", "fields")
lapply(packages, require, character.only = TRUE)
#Source dependencies
#source('/home/ucabchu/Scratch/kidswithgrids/models.R')
source('models.R')
rm(list=ls()) #house keeping
#load packages
packages <- c('plyr', 'jsonlite', 'DEoptim', "matrixcalc", "fields")
lapply(packages, require, character.only = TRUE)
#Source dependencies
#source('/home/ucabchu/Scratch/kidswithgrids/models.R')
source('models.R')
batchName = 'bmtrecovery'
clusterid <- 14
kernellist<-list(rbf, bayesianMeanTracker)
kernelnames<-c("RBF", "BMT")
acqlist<-list(ucb)
acqnames<-c("UCB")
combs<-expand.grid(1:length(kernellist), 1:length(acqlist))
#create a matrix with  combinations of subjectIds and model combinations
subjectComb <- expand.grid(1:160, 1:(length(kernellist) * length(acqlist))) #1:? defines the number of unique models to be analyzed
subjectId <- subjectComb[clusterid,1] #used to identify unique subjects
combId <- subjectComb[clusterid,2] #used to identify a unique model (i.e., combination of GP kernel and acquisition function)
#trim combs if cluster==TRUE
model <- combs[combId,]
set.seed(clusterid) #set seed as the clusterid
#only keep people who have completed the task
data <-read.csv("kwgdata.csv")
#sourced from dataMunging.R
#Normalize data
data$z <- (data$z - 25) / 50
data
modelFit<-function(par, subjD, acquisition, k,  horizonLength, rounds){
#Extract and process parameters
par<-exp(par) #exponentiate parameters to make a non-negative and convex optimization surface
#last parameter is always inverse temperature for softmax
tau<-par[length(par)]
#Which posterior function to use; therefore, which parameters to use
if (inherits(k, "KalmanFilter")){ #null kernel indicates kalman filter model
kNoise <- par[1]
parVec <- c(kNoise) #Vector of parameters to send to the KF posterior function
}else if(inherits(k, "GP")){ #lambda
lambda <- par[1]
parVec <- c(lambda, lambda, 1, .0001) # Vector of parameters to send to the GP posterior vector, where sF and sN are fixed
}
#Additional acquisition function dependent parameters
if (inherits(acquisition, "UCB")){ #check if UCB is used
beta <- par[length(par)-1] #If UCB, beta is always 2nd last
#refactor beta and tau into gamma and beta_star, where gamma = 1/tau and beta_star = beta/tau
}
#which rounds to consider?
trainingSet <- subset(subjD, round %in% rounds)
#Vector to store negative log likelihods
nLL <- rep(0,length(rounds))
for (r in unique(trainingSet$round)){ #Begin looping through each round
#subset of data for round r
roundD <- subset(subjD, round==r)
#is this round a short or long horizon?
horizon <- nrow(roundD)
#Observations of subject choice behavior
chosen <- roundD$chosen
chosen <- chosen[2:length(chosen)] # trim first observation, since it wasn't a choice but a randomly revealed tile
y  <- roundD$z[1:horizon] #trim off the last observation, because it was not used to inform a choice (round already over)
x1 <- roundD$x[1:horizon]
x2 <- roundD$y[1:horizon]
#create observation matrix
X<-as.matrix(cbind(x1,x2))
#initialize Xtest
Xnew<-as.matrix(expand.grid(0:7,0:7))
#make sure X is a matrix
X<-as.matrix(X)
Xnew<-as.matrix(Xnew)
#Utilties of each choice
utilities <- NULL
prevPost <- NULL #set the previous posterior computation to NULL for the kalman filter
#loop through observations
for (i in 1:horizon){ #skip the last observation, because no choice was made based on that information
#new observation
X1<-matrix(X[1:i,], ncol=2)
y1<-y[1:i]
#Which posterior function to use
if (inherits(k, "KalmanFilter")){# kalman filter model
out<- bayesianMeanTracker(x = X1[i,], y=y[i], prevPost = prevPost, theta = parVec)
#update prevPost for the next round
prevPost <- out
}else{# GP with length-scale parameterized kernel
out <- gpr(X.test=Xnew, theta=parVec, X=X1, Y=y1, k=k) #Mu and Sigma predictions for each of the 121 arms; either GP or Kalman filter
}
#Slightly different function calls for each acquisition function
if (inherits(acquisition, "UCB")){ #UCB takes a beta parameter
if (FALSE){
utilityVec <- acquisition(out, c(gamma, beta_star))
} else{
utilityVec<-acquisition(out, c(beta))
}
} else{ #PMU or any other
utilityVec <- acquisition(out)
}
utilityVec <- utilityVec - max(utilityVec) #avoid overflow
utilities <- rbind(utilities, t(utilityVec)) # build horizon_length x 121 matrix, where each row holds the utilities of each choice at each decision time in the search horizon
}
print(utilities)
#Softmax rule
p <- exp(utilities/tau)
p <- p/rowSums(p)
#avoid underflow by setting a floor and a ceiling
p <- (pmax(p, 0.00001))
p <- (pmin(p, 0.99999))
#Calculate Negative log likelihood
nLL[which(unique(trainingSet$round)==r)] <- -sum(log(p[cbind(c(1:(horizon-1)),chosen)]))
}
#end loop through rounds
return(sum(nLL))  #Return negative log likelihoods of all observations
}
cvfun<-function(selector, kernelFun, acquisition, leaveoutindex){
#subselect participant, horizon and rounds not left out
d1<-subset(data, id==selector)
#training set
rounds <- count(d1$round)[count(d1$round)$freq>=2,]$x #only rounds where at least 2 clicks have been made
trainingSet <- rounds[! rounds==leaveoutindex] #remove round specified by leaveoutindex
#test set
testSet <- leaveoutindex
nParams <- 1
if (inherits(acquisition, 'UCB')){
nParams <- nParams + 1 #add beta parameter
}
nParams <- nParams + 1 #add one for tau, which is in all models
#Set upper and lower bounds based on nParams
lbound <- rep(-5, nParams)
ubound <- rep(5, nParams)
#Begin cross validation routine
if (nParams>=2){#if 2 or more parameters
#TRAINING SET
fit<-DEoptim(modelFit, lower=lbound, upper=ubound, subjD=d1, k=kernelFun, rounds = trainingSet, acquisition=acquisition, DEoptim.control(itermax=100))
paramEstimates <- fit$optim$bestmem #MODEL DEPENDENT PARAMETER ESTIMATES
#TEST SET
predict <- modelFit(par=paramEstimates, subjD=d1, acquisition=acquisition, k=kernelFun, rounds=testSet)
output <- c(leaveoutindex, predict, fit$optim$bestmem) # leaveoutindex, nLL, parameters....
} else{
#TRAINING SET
fit<-optimize(modelFit, lower=lbound, upper=ubound, subjD=d1, k=kernelFun, rounds = trainingSet, acquisition=acquisition)
paramEstimates <- fit$minimum #MODEL DEPENDENT PARAMETER ESTIMATES
#TEST SET
predict <- modelFit(par=paramEstimates, subjD=d1, acquisition=acquisition, k=kernelFun, rounds=testSet)
output <- c(leaveoutindex, predict, fit$minimum) #leaveoutindex, nLL, parameters....
}
return(output) #return optimized value
}
output <- c()
#rounds where at least two clicks are conducted
subjdata <- subset(data, id==subjectId)
roundList <- count(subjdata$round)[count(subjdata$round)$freq>=2,]$x
subjdata
count(subjdata$round)[count(subjdata$round)$freq>=2,]$x
unique(subjdata$round)
roundList <- unique(subjdata$round)
r <- 5
cv <- cvfun(subjectId, kernelFun=kernellist[[model[[1]]]], acquisition = acqlist[[model[[2]]]], leaveoutindex=r)
name<-paste0("modelResults/", batchName, kernelnames[model[[1]]], acqnames[model[[2]]], subjectId)
name
name<-paste0("modelResults/", batchName, "/",kernelnames[model[[1]]], acqnames[model[[2]]], subjectId)
name
name<-paste0("modelResults/", batchName, "/",kernelnames[model[[1]]], acqnames[model[[2]]], subjectId, ".csv")
name
data
unique(data$id)
