#Set upper and lower bounds based on nParams
lbound <- rep(-5, nParams)
ubound <- rep(5, nParams)
#Begin cross validation routine
if (nParams>=2){#if 2 or more parameters
#TRAINING SET
fit<-DEoptim(modelFit, lower=lbound, upper=ubound, subjD=d1, k=kernelFun, rounds = trainingSet, acquisition=acquisition, DEoptim.control(itermax=100))
paramEstimates <- fit$optim$bestmem #MODEL DEPENDENT PARAMETER ESTIMATES
#TEST SET
predict <- modelFit(par=paramEstimates, subjD=d1, acquisition=acquisition, k=kernelFun, rounds=testSet)
output <- c(leaveoutindex, predict, fit$optim$bestmem) # leaveoutindex, nLL, parameters....
} else{
#TRAINING SET
fit<-optimize(modelFit, lower=lbound, upper=ubound, subjD=d1, k=kernelFun, rounds = trainingSet, acquisition=acquisition)
paramEstimates <- fit$minimum #MODEL DEPENDENT PARAMETER ESTIMATES
#TEST SET
predict <- modelFit(par=paramEstimates, subjD=d1, acquisition=acquisition, k=kernelFun, rounds=testSet)
output <- c(leaveoutindex, predict, fit$minimum) #leaveoutindex, nLL, parameters....
}
return(output) #return optimized value
}
output <- c()
#rounds where at least two clicks are conducted
subjdata <- subset(data, id==subjectId)
roundList <- count(subjdata$round)[count(subjdata$round)$freq>=2,]$x
subjdata
count(subjdata$round)[count(subjdata$round)$freq>=2,]$x
unique(subjdata$round)
roundList <- unique(subjdata$round)
r <- 5
cv <- cvfun(subjectId, kernelFun=kernellist[[model[[1]]]], acquisition = acqlist[[model[[2]]]], leaveoutindex=r)
name<-paste0("modelResults/", batchName, kernelnames[model[[1]]], acqnames[model[[2]]], subjectId)
name
name<-paste0("modelResults/", batchName, "/",kernelnames[model[[1]]], acqnames[model[[2]]], subjectId)
name
name<-paste0("modelResults/", batchName, "/",kernelnames[model[[1]]], acqnames[model[[2]]], subjectId, ".csv")
name
data
unique(data$id)
setwd('modelResults/bmtrecovery')
d<-read.csv("BMTUCB1.csv")
for (i in 2:160){
d<-rbind(d, read.csv(paste0("BMTUCB", i, ".csv")))
}
d2 <- read.csv('RBFUCB1.csv'
)
for (i in 2:160){
d2<-rbind(d2, read.csv(paste0("RBFUCB", i, ".csv")))
}
d <- rbind(d,d2)
d
d<-read.csv("BMTUCB1.csv")
for (i in 2:160){
d<-rbind(d, read.csv(paste0("BMTUCB", i, ".csv")))
}
write.csv(d, "bmtrecoverybmt.csv")
d2 <- read.csv('RBFUCB1.csv')
for (i in 2:160){
d2<-rbind(d2, read.csv(paste0("RBFUCB", i, ".csv")))
}
write.csv(d2, "gprecoverybmt.csv")
d<-read.csv("BMTUCB1.csv")
for (i in 2:160){
d<-rbind(d, read.csv(paste0("BMTUCB", i, ".csv")))
}
d2 <- read.csv('RBFUCB1.csv')
for (i in 2:160){
d2<-rbind(d2, read.csv(paste0("RBFUCB", i, ".csv")))
}
setwd('..')
setwd('..')
write.csv(d, "bmtrecoverbmt.csv")
write.csv(d2, "gprecoverbmt.csv")
d1<-read.csv("gprecovergp.csv")
d2<-read.csv("bmtrecovergp.csv")
d3<-read.csv("gprecoverbmt.csv")
d4<-read.csv("bmtrecoverbmt.csv")
d1$r2<-1-d1$X.2/(-25*log(1/64))
d2$r2<-1-d2$X.2/(-25*log(1/64))
d3$r2<-1-d3$X.2/(-25*log(1/64))
d4$r2<-1-d4$X.2/(-25*log(1/64))
d1$id<-d2$id<-d3$id<-d4$id<-rep(1:160, each=8)
library(plyr)
dd1<-ddply(d1, ~id, summarize, r2=mean(r2))
dd2<-ddply(d2, ~id, summarize, r2=mean(r2))
dd3<-ddply(d3, ~id, summarize, r2=mean(r2))
dd4<-ddply(d4, ~id, summarize, r2=mean(r2))
dd<-rbind(dd1, dd2, dd3, dd4)
dd$model<-rep(c("GP", "MT", "GP", "MT"), each=nrow(dd1))
dd$producing<-rep(c("GP", "MT"), each=nrow(dd1)*2)
#data frame with everything we need, i.e. min, max, lower nad upper quantile, median, CIs & mean
dp1<-ddply(dd, ~producing+model, summarize,
d_ymin = max(min(r2), quantile(r2, 0.25) - 1.5 * IQR(r2)),
d_ymax = min(max(r2), quantile(r2, 0.75) + 1.5 * IQR(r2)),
d_lower = quantile(r2, 0.25),  d_middle = median(r2), d_upper = quantile(r2, 0.75),
mu=mean(r2))
dp1$model<-factor(dp1$model, levels=(c("GP", "MT")))
dd$model<-factor(dd$model, levels=(c("GP", "MT")))
library(ggplot2)
fontsize<-14
cbPalette <- c("#000000", "#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
names(dd)
dd<-ddply(dd, ~id+model+producing, r2=mean(r2))
p1<-ggplot(data = dp1) +
#boxplot with given values, we only need half of it
geom_boxplot(aes(x = as.numeric(model)-0.2, ymin = d_lower, ymax = d_upper, lower = d_lower,
middle = d_middle, upper = d_upper, width = 2 * 0.2, fill = model), stat = "identity") +
#jitter of raw data points, needs the full data frame
geom_jitter(data=dd, aes(x = as.numeric(model) + 0.2,  y = r2,  color = model),
width = 0.2 - 0.25 * 0.2, height = 0, size=1)+
#vertical segment
geom_segment(aes(x = as.numeric(model), y = d_ymin, xend = as.numeric(model), yend = d_ymax)) +
geom_point(aes(x = as.numeric(model)-0.2, y = mu), shape=23, size=3, fill="white", color="black") +
#top horizontal segment
geom_segment(aes(x = as.numeric(model) - 0.1,  y = d_ymax, xend = as.numeric(model),  yend = d_ymax)) +
#top vertical segment
geom_segment(aes(x = as.numeric(model) - 0.1, y = d_ymin, xend = as.numeric(model), yend = d_ymin)) +
facet_wrap(~producing)+
#theme minimal
theme_minimal()+
#sans
theme(text = element_text(size=fontsize,  family="sans"))+
#colors and fill
scale_fill_manual(values = c(cbPalette[c(4,5)]))+
scale_color_manual(values = c(cbPalette[c(4,5)]))+
#labs
xlab("Model")+ylab(expression("Predictive accuracy"~r^2))+
#no legend
theme(legend.position="none", strip.background=element_blank(), legend.key=element_rect(color=NA))+
#labe x-axis
scale_x_continuous(breaks = c(1,2,3,4),labels = c("GP","MT", "GP", "MT"))+
ggtitle("Recovery")+
#various theme changes including reducing white space and adding axes
theme(axis.line.x = element_line(color="grey20", size = 1),
axis.line.y = element_line(color="grey20", size = 1),
panel.spacing.x=unit(0.2, "lines"),
panel.spacing.y=unit(1, "lines"),
plot.title = element_text(family = "sans", margin=margin(0,0,0,0)),
plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"))
pdf("recoverycompare.pdf", width=8, height=4)
p1
dev.off()
mm<-read.csv("bmtrecoverbmt.csv")
mg<-read.csv("bmtrecovergp.csv")
gm<-read.csv("gprecoverbmt.csv")
gg<-read.csv("gprecovergp.csv")
mm$r2<-(1-mm$X.2/(-26*log(1/64)))
mg$r2<-(1-mg$X.2/(-26*log(1/64)))
gm$r2<-(1-gm$X.2/(-25*log(1/64)))
gg$r2<-(1-gg$X.2/(-25*log(1/64)))
mean(mm$r2)
mean(gm$r2)
mean(gg$r2)
mean(mg$r2)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
library(plyr)
library(BayesFactor)
library(lsr)
mm<-ddply(mm,  ~id,summarize, m=median(r2))
gm<-ddply(gm,  ~id,summarize, m=median(r2))
mg<-ddply(mg,  ~id,summarize, m=median(r2))
gg<-ddply(gg,  ~id,summarize, m=median(r2))
mean(gm$m)
t.test(mm$m-gm$m)
cohensD(mm$m-gm$m)
ttestBF(mm$m-gm$m)
sum(mm$m-gm$m>=0)
t.test(gg$m-mg$m)
cohensD(gg$m-mg$m)
ttestBF(mm$m-gm$m)
sum(gg$m-mg$m>=0)
t.test(mm$m-gm$m)
ttestBF(mm$m-gm$m)
mean(gm$m)
mean(mm$m)
t.test(mm$m-gm$m)
cohensD(mm$m-gm$m)
install.packages('BayesFactor')
library(BayesFactor)
ttestBF(mm$m-gm$m)
sum(mm$m-gm$m>=0)
mm$m-gm$m
sum(mm$m-gm$m>=0)
sum(gg$m-mg$m>=0)
sum(mm$m-gm$m>=0)
mm<-read.csv("bmtrecoverbmt.csv")
mg<-read.csv("bmtrecovergp.csv")
gm<-read.csv("gprecoverbmt.csv")
gg<-read.csv("gprecovergp.csv")
mm$r2<-(1-mm$X.2/(-25*log(1/64)))
mg$r2<-(1-mg$X.2/(-25*log(1/64)))
gm$r2<-(1-gm$X.2/(-25*log(1/64)))
gg$r2<-(1-gg$X.2/(-25*log(1/64)))
mean(mm$r2)
mean(gm$r2)
mean(gg$r2)
mean(mg$r2)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
library(plyr)
library(BayesFactor)
library(lsr)
mm<-ddply(mm,  ~id,summarize, m=median(r2))
gm<-ddply(gm,  ~id,summarize, m=median(r2))
mg<-ddply(mg,  ~id,summarize, m=median(r2))
gg<-ddply(gg,  ~id,summarize, m=median(r2))
mean(gm$m)
mean(mm$m)
t.test(mm$m-gm$m)
cohensD(mm$m-gm$m)
ttestBF(mm$m-gm$m)
sum(mm$m-gm$m>=0)
t.test(gg$m-mg$m)
cohensD(gg$m-mg$m)
ttestBF(mm$m-gm$m)
sum(gg$m-mg$m>=0)
mm$X.2
mm<-read.csv("bmtrecoverbmt.csv")
mg<-read.csv("bmtrecovergp.csv")
gm<-read.csv("gprecoverbmt.csv")
gg<-read.csv("gprecovergp.csv")
mm$X.2
mm$r2<-(1-mm$X.2/(-25*log(1/64)))
mm$r2
mg$r2<-(1-mg$X.2/(-25*log(1/64)))
gm$r2<-(1-gm$X.2/(-25*log(1/64)))
gg$r2<-(1-gg$X.2/(-25*log(1/64)))
mean(mm$r2)
mean(gm$r2)
mean(gg$r2)
mean(mg$r2)
mean(gm$r2)
mean(mg$r2)
mean(mm$r2)
mean(gm$r2)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
mm<-read.csv("bmtrecoverbmt.csv")
mm
mm<-ddply(mm,  ~id,summarize, ll=median(X.2)) #calculate log os
mm<-ddply(mm,  ~id,summarize, ll=sum(X.2)) #calculate log os
mm<-ddply(mm,  ~id,summarize, ll=median(X.2))
mm$id
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
mm$id
mm<-ddply(mm,  ~id,summarize, ll=sum(X.2))
mm
log(1/64)
mm$r2<-(1-mm$ll/(-8*25*log(1/64))) #random chance is 1/64 for each trial, for 25 trials, in 8 rounds
mg$r2<-(1-mg$ll/(-8*25*log(1/64)))
gm$r2<-(1-gm$ll/(-8*25*log(1/64)))
gg$r2<-(1-gg$ll/(-8*25*log(1/64)))
(-8*25*log(1/64)
(-8*25*log(1/64))
(-8*25*log(1/64)))
(-8*25*log(1/64)))
(-8*25*log(1/64))
mm$r2<-(1-mm$ll/(-8*25*log(1/64)))
mg$r2<-(1-mg$ll/(-8*25*log(1/64)))
mg<-ddply(mg,  ~id,summarize, ll=sum(X.2))
library(plyr)
library(BayesFactor)
library(lsr)
#Read recovery data files
mm<-read.csv("bmtrecoverbmt.csv")
mg<-read.csv("bmtrecovergp.csv")
gm<-read.csv("gprecoverbmt.csv")
gg<-read.csv("gprecovergp.csv")
#
#mean(mm$r2)
#mean(gm$r2)
#mean(gg$r2)
#mean(mg$r2)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
#sum log loss over 8 crossvalidation slices
mm<-ddply(mm,  ~id,summarize, ll=sum(X.2))
gm<-ddply(gm,  ~id,summarize, ll=sum(X.2))
mg<-ddply(mg,  ~id,summarize, ll=sum(X.2))
gg<-ddply(gg,  ~id,summarize, ll=sum(X.2))
#compute R2
mm$r2<-(1-mm$ll/(-8*25*log(1/64))) #random chance is 1/64 for each trial, for 25 trials, in 8 rounds
mg$r2<-(1-mg$ll/(-8*25*log(1/64)))
gm$r2<-(1-gm$ll/(-8*25*log(1/64)))
gg$r2<-(1-gg$ll/(-8*25*log(1/64)))
mean(gm$m)
mean(mm$m)
t.test(mm$m-gm$m)
cohensD(mm$m-gm$m)
ttestBF(mm$m-gm$m)
sum(mm$m-gm$m>=0)
library(plyr)
library(BayesFactor)
library(lsr)
#Read recovery data files
mm<-read.csv("bmtrecoverbmt.csv")
mg<-read.csv("bmtrecovergp.csv")
gm<-read.csv("gprecoverbmt.csv")
gg<-read.csv("gprecovergp.csv")
#
#mean(mm$r2)
#mean(gm$r2)
#mean(gg$r2)
#mean(mg$r2)
mm$id<-mg$id<-gm$id<-gg$id<-rep(1:160, each=8)
#sum negative log likelihood over 8 crossvalidation slices
mm<-ddply(mm,  ~id,summarize, nll=sum(X.2))
gm<-ddply(gm,  ~id,summarize, nll=sum(X.2))
mg<-ddply(mg,  ~id,summarize, nll=sum(X.2))
gg<-ddply(gg,  ~id,summarize, nll=sum(X.2))
#compute R2
mm$r2<-(1-mm$nll/(-8*25*log(1/64))) #random chance is 1/64 for each trial, for 25 trials, in 8 rounds
mg$r2<-(1-mg$nll/(-8*25*log(1/64)))
gm$r2<-(1-gm$nll/(-8*25*log(1/64)))
gg$r2<-(1-gg$nll/(-8*25*log(1/64)))
mean(gm$nll)
mean(mm$r2)
mean(gm$r2)
t.test(mm$r2-gm$r2)
cohensD(mm$r2-gm$r2)
ttestBF(mm$nll-gm$nll)
ttestBF(mm$r2-gm$nr2)
ttestBF(mm$r2-gm$r2)
sum(mm$r2-gm$r2>=0)
sum(mm$r2-gm$r2>=0) #how many simualted participants are best described by BMT vs. GP
sum(mm$r2-gm$r2=0) #how many simualted participants are best described by BMT vs. GP
sum(mm$r2-gm$r2==0)
sum(mm$r2-gm$r2>=0)
t.test(gg$r2-mg$r2)
cohensD(gg$r2-mg$r2)
cohensD(mm$r2-gm$r2)
ttestBF(mm$r2-gm$r2)
sum(gg$r2-mg$r2>=0)
#house keeping
rm(list=ls())
#packages
packages <- c('plyr', 'ggplot2', 'jsonlite', 'BayesFactor', 'lsr')
lapply(packages, require, character.only = TRUE)
#read in data
myjson<-fromJSON("kwg.json")
#RBF-results
drbf<-read.csv("rbf.csv")
#meant tracker results
dbmt<-read.csv("bmt.csv")
#get pseudo-r2
drbf$r2<-(1-drbf$X.2/(-25*log(1/64)))
dbmt$r2<-(1-dbmt$X.2/(-25*log(1/64)))
drbf$r2
dat
#house keeping
rm(list=ls())
#packages
packages <- c('plyr', 'ggplot2', 'jsonlite', 'gridExtra', 'ggjoy', "tikzDevice")
lapply(packages, require, character.only = TRUE)
#read in data
myjson<-fromJSON("kwg.json")
#empty frame
dat<-data.frame(id=numeric(), cond=numeric(), age=numeric(),
x=numeric(), y=numeric(), z=numeric(), time=numeric(),
trial=numeric(), round=numeric())
myjson$count
)i <- 1
myjson$records
myjson$records$data
myjson$records$data$searchHistory$xcollect
myjson$records$data$searchHistory$xcollect[[i]][2:9,]
i <- 1
myjson$records$data$searchHistory$xcollect[[i]][2:9,]
myjson$records$data$searchHistory$xcollect[[i]][2:9,]
x<-as.vector(t(myjson$records$data$searchHistory$xcollect[[i]][2:9,]))
x
#we need rounds 2-9
x<-as.vector(t(myjson$records$data$searchHistory$xcollect[[i]][2:9,]))
y<-as.vector(t(myjson$records$data$searchHistory$ycollect[[i]][2:9,]))
z<-as.vector(t(myjson$records$data$searchHistory$zcollect[[i]][2:9,]))
time<-as.vector(t(myjson$records$data$searchHistory$tscollect[[i]][2:9,]))
cond<-rep(myjson$records$data$condition[i], length(x))
age<-rep(myjson$records$data$age[i], length(x))
trial<-rep(0:25, 8)
round<-rep(1:8, each=26)
id<-rep(i, length(x))
#get the dummy frame
dummy<-data.frame(id=id, cond=cond, age=age, x=x, y=y, z=z, time=time, trial=trial, round=round)
dummy
#loop through participants
for (i in 1:myjson$count){
#we need rounds 2-9
x<-as.vector(t(myjson$records$data$searchHistory$xcollect[[i]][2:9,]))
y<-as.vector(t(myjson$records$data$searchHistory$ycollect[[i]][2:9,]))
z<-as.vector(t(myjson$records$data$searchHistory$zcollect[[i]][2:9,]))
time<-as.vector(t(myjson$records$data$searchHistory$tscollect[[i]][2:9,]))
cond<-rep(myjson$records$data$condition[i], length(x))
age<-rep(myjson$records$data$age[i], length(x))
trial<-rep(0:25, 8)
round<-rep(1:8, each=26)
id<-rep(i, length(x))
#get the dummy frame
dummy<-data.frame(id=id, cond=cond, age=age, x=x, y=y, z=z, time=time, trial=trial, round=round)
#conctatenate
dat<-rbind(dat, dummy)
}
#first 10 entries are us!
dat<-subset(dat, id>10)
#kids younger than 7 are not allowed
dat<-subset(dat, age>=7)
#fontsize is 8
fontsize<-14
#standard error
se<-function(x){sd(x)/sqrt(length(x))}
#age groups are 7-8, 9-11, and adults
dat$agegroup<-ifelse(dat$age<9, "7-8", dat$age)
dat$agegroup<-ifelse(dat$age>=9 & dat$age <12, "9-11", dat$agegroup)
dat$agegroup<-ifelse(dat$age>18, ">18", dat$agegroup)
#conditions are smooth and rough
dat$Condition<-ifelse(dat$cond==1, "Rough", "Smooth")
#bootstrapped upper CI
upci<-function(x, id){
x<-ddply(data.frame(id,x), ~id, summarize, x=mean(x))$x
return(mean(x)+2.14*(sd(x)/sqrt(length(x))))
}
#bootstrapped lower ci
downci<-function(x, id){
x<-ddply(data.frame(id,x), ~id, summarize, x=mean(x))$x
mean(x)-2.14*(sd(x)/sqrt(length(x)))
}
dat$Age<-dat$agegroup
#data frame with everything we need, i.e. min, max, lower nad upper quantile, median, CIs & mean
dp1<-ddply(subset(dat,trial>0), ~Condition+Age, summarize,
d_ymin = max(min(z), quantile(z, 0.25) - 1.5 * IQR(z)),
d_ymax = min(max(z), quantile(z, 0.75) + 1.5 * IQR(z)),
d_lower = quantile(z, 0.25),  d_middle = median(z), d_upper = quantile(z, 0.75),
mu=mean(z))
dp1
#housekeeping
rm(list=ls())
#source of modeling code
source("models.R")
#packages
packages <- c('plyr', 'ggplot2', 'jsonlite', 'lsr', 'BayesFactor', 'matrixcalc')
#load them
lapply(packages, require, character.only = TRUE)
#two environments
environments1 <- fromJSON("kernelRough.json")
environments2 <- fromJSON("kernelSmooth.json")
roughEnvironments <- lapply(environments1, FUN=function(x) matrix(as.numeric(unlist(x)), ncol=3, byrow=TRUE, dimnames=list(seq(1,64), c('x2', 'y', 'x1'))))
smoothEnvironments <- lapply(environments2, FUN=function(x) matrix(as.numeric(unlist(x)), ncol=3, byrow=TRUE, dimnames=list(seq(1,64), c('x2', 'y', 'x1' ))))
roughEnvironments
rough<-as.data.frame(roughEnvironments[[1]])
rough
rough<-as.data.frame(roughEnvironments[[1]])
smooth<-as.data.frame(smoothEnvironments[[1]])
for (i in 2:40){
rough<-rbind(rough,as.data.frame(roughEnvironments[[i]]))
smooth<-rbind(smooth,as.data.frame(smoothEnvironments[[i]]))
}
rough$en<-smooth$en<-rep(1:40, each=64)
rough$y
rough$y*35+5
mean(rough$y*35+5)
hist(rough$y*35+5)
hist(rough$y)
myjson$records$data$scale[[k]]
rough$y
rough$y<-rough$y*35+5
smooth$y<-smooth$y*35+5
dparams<-expand.grid(lambda=seq(0.1,4,0.1), beta=seq(0.1,4,0.1), tau=0.03)
dparams$smooth<-dparams$rough<-0
Xstar<-cbind(smooth$x1[1:64], smooth$x2[1:64])
k<-rbf
nrow(dparams)
i <- 1
lambda<-dparams$lambda[i]
beta<-dparams$beta[i]
tau<-dparams$tau[i]
parVec <- c(lambda, lambda, 1, .0001)
parVec
musmooth<-murough<-numeric()
enselect<-sample(1:40, 1)
environ<-subset(smooth, en==enselect)
ind<-sample(1:64,1)
#X matrix
X<-cbind(environ$x1[ind], environ$x2[ind])
#y matrix
y<-as.matrix(environ$y[ind])
#loop through trials
for (trial in 1:25){
#output by GP with particular parameter settings
#don't forget mean centering and standardization
out<-gpr(X.test=Xstar, theta=parVec, X=X, Y=(y-25)/50, k=k)
#utility vector by UCB
utilityVec<-ucb(out, beta)
#avoid overflow
utilities <- utilityVec - max(utilityVec)
#softmaximization
p <- exp(utilities/tau)
#probabilities
p <- p/colSums(p)
#numerical overflow
p <- (pmax(p, 0.00001))
p <- (pmin(p, 0.99999))
#index is sampled proprotionally to softmaxed utitily vector
ind<-sample(1:64, 1, prob=p)
#bind X-observations
X<-rbind(X, cbind(environ$x1[ind], environ$x2[ind]))
#bind y-observations
y<-rbind(y, as.matrix(environ$y[ind]))
}
enselect
environ
smooth
X
y
Xstar
